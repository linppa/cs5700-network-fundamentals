#!/usr/bin/env python3
# ssh linppa@login-students.khoury.northeastern.edu
# Your password is: b4da98eadabe2ce0801f46a20b35f587a70128969ec5cba5e0535f1f4ad7b076

import argparse
import socket
import ssl

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443

class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password

        # TODO: track the frontier; store uncrawled url in queue, stack, or list
        # TODO: track the history; store crawled url
        # TODO: only crawl target domain

    def run(self):
        # TODO: HTTP/1.1; supports chunked encoding
        # TODO: 'connection: keep-alive' tricky to get correct, 'accept-encoding: gzip'
        request = "GET /accounts/login/?next=/fakebook/ HTTP/1.1\r\nHost: proj5.3700.network\r\n\r\n"

        print("Request to %s:%d" % (self.server, self.port))
        print(request)
        
        # create my socket
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        mysocket.connect((self.server, self.port))

        # implement TLS encryption
        tls_encrypt = ssl.create_default_context()
        mysocket = tls_encrypt.wrap_socket(mysocket, server_hostname=self.server)

        # send the request
        mysocket.send(request.encode('ascii'))

        # receive the full html response
        data = mysocket.recv(1000).decode('ascii')
        while '</html>' not in data:
            if not data:
                break
            data += mysocket.recv(1000).decode('ascii')
        print(f'** full response; **\n{data}')

        # TODO: HTTP protocol aspects; GET, POST, store cookies + submit with GET request
        # TODO: handle status codes; 
            # 200 - OK
            # 302 - Found
            # 403 - Forbidden & 404 - Not Found
            # 503 - Service Unavailable - retry request until successful
        

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
